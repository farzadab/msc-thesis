\chapter{Conclusions}
\label{ch:conclusions}

Reinforcement learning provides a promising new path for motion generation, one that can be generalized to new terrains and character morphologies. However the current methods are computationally inefficient and unless motion captured data is used, the motion quality is typically unsatisfactory for applications in computer graphics.

Our work  tackles these two problems by investigating two issues: excessively large torque limits and gait asymmetry. We show that more realistic torque limits, though resulting in more natural motions, can hinder the training in the beginning. We propose to use a simple curriculum learning technique that starts with higher torque limits to speed up the training but gradually decreases the limits to arrive at more natural final motions. This way we get the best of both worlds.

Next, we looked at ways of incorporating gait symmetry into the training process. Symmetric motions are generally perceived to be more attractive in humans and asymmetric patterns are commonly associated with disability or injury. We have compared four methods of enforcing symmetry in various environments as well as discussing their advantages and drawbacks in different scenarios.

As with any other work, there remain some questions to be answered.
I would be interested in using the torque limit curriculum approach to figure out what is the lowest torque limit that still permits locomotion in a robot, such as Cassie. This can help with designing cheaper robots.

Another promising direction is to look at the time-step used for locomotion. In all of our work, we used a prespecified control frequency. Humans, however, tend to plan at different time horizons depending on the task and their skill level. We have promising initial results showing that using curriculum learning to transfer skills learned with a specific control frequency to another can improve performance. Perhaps the reason for this is that the agent can better focus on the long-term goals while still having the opportunity to refine its movements. This can help bridge the gap between low-level skill learning and long-term planning. This idea is also closely related to hierarchical reinforcement learning.