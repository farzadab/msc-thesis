\chapter{Related Work}
\label{ch:relatedwork}


\section{Motion Symmetry}
Motion symmetry has been a topic of interest for many years in the study of human motion and movement biomechanics. 
Symmetric motions are perceived to be more attractive, e.g., for dance~\cite{danceSymmetry},
and gait symmetry is seen an as a desirable outcome for physiological manipulation~\cite{robinson1987use}.
While symmetry is a common assumption in the study of gait and posture, 
individual gaits often do exhibit asymmetries due to various possible functional causes~\cite{seelet}.
We refer the reader to a past review article~\cite{SADEGHI200034} for insights
into the degree of symmetry of lower limbs movement during able-bodied gait
and the potential influence of limb dominance on the motion symmetry of the lower extremities.
and human gaits~\cite{riskowski}.
It is also not obvious how to best quantify 
the asymmetry of human gaits, and thus specific symmetry metrics have been
proposed~\cite{symmetry_measures,symmetry_phase_portrait}.

The robust control of physics-based character locomotion has been a long-standing challenge
for character animation. We refer the reader to a survey paper for a detailed history~\cite{STAR2012}.  
An early and enduring approach to controller design has been to structure control policies around finite state
machines (FSMs) and feedback rules that use a simplified abstract model or feedback law.  These general
ideas have been applied to human athletics and running~\cite{Hodgins95} and a rich variety of walking
styles~\cite{Yin07,Coros10,LeeYS10}. Many controllers developed for physics-based animation
further use optimization methods to improve controllers developed around an FSM-structure, or use an FSM to 
define phase-dependent objectives for an inverse dynamics optimization to be solved at each time step.
Policy search methods, e.g., stochastic local search or CMA~\cite{Hansen06}, can be used to optimize the 
parameters of the given control structures to achieve a richer variety of motions, e.g.,~\cite{Yin07,Coros11}, and
efficient muscle-driven locomotion~\cite{Wang09}.  
Many of the FSM controllers use hard-coded symmetries, which assign the roles of stance-leg or swing-leg
to the left and right legs, as a function of the FSM state.
It is common in kinematic-based approaches to locomotion, e.g.,~\cite{bruderlin,HoldenPFNN}, 
to also mirror all the available motion data
in order to double the effective size of the data set, and to reflect the often-symmetric nature of human locomotion.
Lastly, trajectory optimization-based methods also commonly assume motion symmetry when convenient, e.g.,~\cite{majkowska2007flipping}.

More recently, locomotion synthesis has attracted significant
attention from the reinforcement learning (RL) community, where the OpenAI Gym tasks have 
become a popular RL benchmark~\cite{ref:OpenAI-Gym}. In this context, symmetry constraints are commonly 
not imposed, and the resulting motions often have noticable asymmetries. 
Further work extends these efforts in a variety of ways, including the traversing challenging terrains~\cite{ref:deepmindParkour}.
More realistic and dynamic motions can be achieved with the help of 
motion-capture clips~\cite{2017-TOG-deepLoco,2018-TOG-deepMimic} and these use
what we refer to as the PHASE symmetry method with the goal of more efficient learning, although with
no robust documented experiments to verify the efficiency gains.
The efficient learning of controllers capable of producing high-quality motion for realistic-strength characters remains
a challenging problem in the absence of motion capture data. Recent work makes
progress on this problem using RL with a combination of energy optimization, learning curriculum, and 
an auxiliary motion symmetry loss~\cite{Yu-SIGGRAPH-2018}, which we shall refer to as the LOSS method.

A recent result investigates how DRL problems can become prone to learning plateaus because
of winner-take-all solution modes~\cite{ray_interf}. These can easily arise in DRL because the distribution of data
for policy learning is directly influenced by the policy itself. Thus in the case of multiple diverging 
decision paths, one of the modes will quickly dominate. The choice of whether to encourage symmetry,
and how to do so, may create an optimization landscape that exhibits similar properties.
