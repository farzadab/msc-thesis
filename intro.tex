%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Introduction}
\label{ch:Introduction}

Locomotion is a long-standing problem in computer graphics.
From the first attempts by using key-framing \red{cite} to the costly motion capture studios, many people have spent hours upon hours trying to make characters move. Recently, \ac{DRL} has emerged as a general framework for the control of physically-simulated movement. It holds significant potential because of its adaptability different tasks and characters.
% achieved via learned control policies that optimize a  reward objective.

The generality of the \ac{DRL} formulation makes it applicable to a wide variety of settings. Recent successes of \ac{DRL} in animation and machine learning show the ability to produce robust learned locomotion for simulated humans, animals, and imaginary legged creatures. However, there remains a compelling need to improve learning efficiency and motion quality in order for \ac{DRL} to become a widely-adopted animation tool which we will describe in more detail.



% \red{From one perspective, this can be seen as substituting animator time or the price of a capturing motion capture data with compute resources, but we should be aware that though the latter might be cheap, it is not infinite.}

The classical control theory studies almost the exact same problem as \ac{RL} but from a different perspective. Control theory usually assumes complete knowledge about the system that is under the investigation and its environment. Even unknown events like disturbances and noise are usually modeled and extra assumptions about their nature are made in this framework \red{including??}. This way a control algorithm, such as LQR \red{CITE}, can solve the task at hand at first try very efficiently. However, as the task becomes more complex or the number of unobserved parameters increase this approach can fail to deliver any solution much less the optimal one.

The current model-free methods attempt to make little to no assumptions about the task at hand. This approach is at direct odds with the classical control theory perspective and it might be responsible for why we still talk about \ac{RL} and control theory as different disciplines. This way \ac{RL} can solve problems at a higher abstraction level than control theory does. This way you get a universal algorithm that can solve many sorts of problems and is highly flexible. However, this generality comes at a price. These algorithms requires countless interactions with the system that is under investigation. To make this more concrete, it is common for a task to be solve after ten million time-steps which translates to roughly fifty hours of non-stop interaction with the character or the robot.

Furthermore, encoding the properties of a desirable motion through the reward signal can prove highly challenging and can fail in undesirable and unintuitive ways. This motivates us to augment the traditional \ac{DRL} formulation with prior knowledge to gain more natural looking motions.

In this work we specifically explore two methods that can help gain higher quality motion as well as faster learning. In \autoref{ch:envparams}, we will look at the effects of using more realistic torque limits on the learning process. We will show that using realistic motions from the start can hinder the training process to the point that the agent never learns to move in the allotted time period. However, we can overcome this by using a simple curriculum schedule.

Next, in \autoref{ch:symmetry}, we will look at symmetry. Symmetric motions are perceived to be more attractive, e.g., for dance \cite{danceSymmetry}, and gait symmetry is seen an as a desirable outcome for physiological manipulation \cite{robinson1987use}. \red{better?}
We will explore four different methods of incorporating symmetry into the \ac{DRL} paradigm and discuss their drawbacks. We then compare their motion quality as well as the degree to which they achieve true gait symmetry in practice.