%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Introduction}
\label{ch:Introduction}

The ability to walk seems so mundane that we usually take it for granted. We can gracefully move around without the need to consciously think about how to coordinate our muscles while walking on uneven terrain. Sadly, we are still unable to create controllers that replicate what a newborn horse can do after two hours of being born. This is not due to a lack of effort as the problem of locomotion has been studied by many researchers in different fields.

Computer graphics got interested in locomotion in order to bring characters to life. We can divide motion generation techniques into two categories: \textit{kinematic} and \textit{dynamic} (physics-based) methods. The kinematic approach tries to produce the desired motion by directly manipulating the object positions and joint angles as well as their respective velocities.
However, modifying these values directly can lead to strange motions that seem unrealistic. One major reason for this is that the resulting motions can sometimes break the laws of physics. We as humans are adept at picking up such inconsistencies.
An intuitive approach to fixing this problem is to use the laws of physics as constraints. This method is known as physics-based animation.


Physics-based animation first requires creating accurate models of objects and articulated bodies by taking into account their parameters such as mass, dimensions, joint types, torque limits, and so on. Then the question becomes, how do we control our characters in order to achieve the desired movements, which we and many others have tried to address.



%%%
 Recently, \ac{RL} has emerged as a promising approach to learning these skills using by using simulated characters. It holds significant potential because of its adaptability different tasks and characters.

\ac{RL} formulates the problem as a control task where the agent can manipulate the muscle activations individually at each time step to achieve a certain goal indicated by a reward signal. The generality of the \ac{RL} formulation makes it applicable to a wide variety of settings. Recent successes of \ac{RL} in animation and machine learning show the ability to produce robust learned locomotion for simulated humans, animals, and imaginary legged creatures. However, there remains a compelling need to improve learning efficiency and motion quality in order for \ac{RL} to become a widely-adopted animation tool which is the goal of this thesis.


% From the first attempts by using key-framing \red{cite} to the costly motion capture studios, many people have spent hours upon hours trying to make characters move.
% achieved via learned control policies that optimize a  reward objective.


% However, as different communities they have separate goals to accomplish. The machine learning community usually treats locomotion tasks as a benchmark for the development and comparison of algorithms \red{CITE  ... ?}. Unfortunately, this means that the quality of motion is generally not a concern unless it is somehow reflected in the reward function. Therefore it is not surprising to see publications with bizarre motions. The robotics community on the other hand cares about the stability and reliability. On the other hand, in computer graphics controllability and the motion quality are of high importance.


% \red{From one perspective, this can be seen as substituting animator time or the price of a capturing motion capture data with compute resources, but we should be aware that though the latter might be cheap, it is not infinite.}

The classical control theory studies almost the exact same problem as \ac{RL} but from a different perspective. Control theory usually assumes complete knowledge about the system that is under the investigation and its environment. Even unknown events like disturbances and noise are usually modeled and extra assumptions about their nature are made in this framework. This way a control algorithm, such as LQR, can solve the task at hand at first try very efficiently. However, as the task becomes more complex or the number of unobserved parameters increases this approach can fail to deliver any solution.

The current model-free methods attempt to make little to no assumptions about the task at hand. This approach is at direct odds with the classical control theory perspective and it might be responsible for why we still talk about \ac{RL} and control theory as different disciplines. This way \ac{RL} can solve problems at a higher abstraction level than control theory does. This approach leads to a universal algorithm that can solve many sorts of problems and is highly flexible. However, this generality comes at a price. These algorithms are inefficient  and require countless interactions with the system that is under investigation. To make this more concrete, it is common for a task to be considered solved after ten million time-steps which translates to roughly fifty hours of non-stop interaction with the character or the robot.

Furthermore, encoding the properties of a desirable motion through the reward signal can prove highly challenging and can fail in undesirable and un-intuitive ways. Common reward functions for walking and running are primarily based on forward progress or a fixed root velocity. Surprisingly, the agent can produce a walk like behavior given little to no extra knowledge. However, the motions are usually far from appealing. Progress being the primary reward signal, the agent tends to learn peculiar motions with the hands flailing around in the air and the head fixed in unnatural positions. Even characters without an upper body commonly find irregular gaits, such as the fencing gait, that keeps one foot in front of the other without switching.

Further engineering the reward, such as including an energy minimization objective, can alleviate some of the problems to some extent, but it can also produce other issues that are difficult to debug, such as creating local minima that the agent struggles to get out of. Therefore, the problems arising from reward engineering approach itself can quickly outweigh the issues that it solves.

This motivates us to look at other ways to augment the traditional \ac{RL} formulation with prior knowledge to gain more natural looking motions. In this work we specifically explore two methods that can help gain higher quality motion as well as faster learning. In \Cref{ch:envparams}, we will look at the effects of using more realistic torque limits on the learning process. We will show that using realistic motions from the start can hinder the training process to the point that the agent never learns to move in the allotted time period. However, we can overcome this by using a simple curriculum schedule.

Next, in \Cref{ch:symmetry}, we will look at another source of low quality in the motions that are generated via the \ac{RL} paradigm, namely, asymmetric walking patterns. The left and the right sides of the human body are fairly symmetric. Consequently, walking patterns of healthy humans are quite symmetric. Symmetric motions are also perceived to be more attractive, e.g., for dance \cite{danceSymmetry}, and gait symmetry is seen an as a desirable outcome for physiological manipulation \cite{robinson1987use}. However, \ac{RL} agents commonly find asymmetric gait patterns such as fencing, which leads with one primary foot and tends not to switch the leading foot. We will explore four different methods of incorporating symmetry into the \ac{RL} paradigm and discuss their advantage and drawbacks. We then compare their motion quality as well as the degree to which they achieve true gait symmetry in practice.